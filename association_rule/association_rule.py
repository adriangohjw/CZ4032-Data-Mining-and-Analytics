from pprint import pprint
from data_processor import load_dataset, clean_dataframe, get_binning_references, bin_dataframe, filter_df_columns, append_col_name_to_dataframe, get_frequent_itemsets, get_association_rules, unmap_classifiers_with_binning_refereces
from classifier_generator import ClassifierGenerator
from rule_selection import RuleSelection
from accuracy_analyzer import AccuracyAnalyzer
from performance_analyzer import get_df_memory_size
import time
import pandas as pd

# Suppress warnings
pd.options.mode.chained_assignment = None

#########################
# Setting up and configurating which dataset and attributes to use
#########################

# DATA_SOURCE is the path to the dataset that we want to evaluate
DATA_SOURCE = 'Adults/data_cleaned.csv'
DELIMITER = ','

# These are all the attributes that will be used to perform the analysis
attributes = [
  'age', 'hours-per-week',
  'workclass', 'education', 'marital-status', 'occupation',
  'relationship', 'race', 'sex', 'native-country', 'income-group'
]

# This is the attribute to be predicted at this field
attribute_to_classify = 'income-group'


#########################
# Data preprocessing and getting it ready for the analysis
#########################

# Import and load the dataset
data = load_dataset(DATA_SOURCE, DELIMITER)

# Remove the attributes that we don't need 
# This will save us time and memory during the computation
data_with_selected_attributes = filter_df_columns(data, attributes)


#########################
# Step 1: Discreitizing continuous attributes
#########################

# binning_references is a dictionary that contains the breaking point between bins
# It is important and will be used to explain the rules generated by the classifer later on
binning_references = get_binning_references(data_with_selected_attributes)
bin_dataframe(data_with_selected_attributes, binning_references)

# Append the attribute name to the dataframe
# This is necessary because the classifier will be trained on the dataframe
# This will help the classifer to differentiate between the values across each attribute
# E.g. value 10 from attribute 'radius_mean' will be mapped to 'radius_mean_10'
df_with_appended_column_names = append_col_name_to_dataframe(data_with_selected_attributes)


#########################
# Setting up tools to help us understand the performance of the classifier
#########################

# Display the memory (In MB) used by the dataframe that we will use to train the classifier
print(get_df_memory_size(df_with_appended_column_names))

# Start the "stopwatch" to measure the time taken to train the classifier
start = time.process_time()


#########################
# Step 2: Building the rule generator (called CBA-RG)
#########################

# Retrieve the frequent itemsets from the dataframe
frequent_items = get_frequent_itemsets(df_with_appended_column_names, min_support=0.3)

# Use the frequent itemsets to generate association rules based on Apriori algorithm
assoc_rules = get_association_rules(frequent_items)

# Load the association rules generated 
# Remove any rule with more than 1 consequents (i.e. more than 1 class)
# This is because we don't want to use rules that have more than 1 class as the consequent
cg = ClassifierGenerator(assoc_rules, attribute_to_classify)
classifiers = cg.classifiers


#########################
# Step 3: Building the classifier (called CBA-CB)
#########################

test_data = data.copy()
bin_dataframe(test_data, binning_references)

# Use the derived rules to guess the class of each instance
# Ranking the rules based on their confidence and support, we will run each rule
# through the data and guess the class for an instance with matching antecedent(s)
# If there's no instance that match the antecedent(s), we will remove the rule
# After testing all the rules, we will guess the remaining instances 
# with the default class (i.e. the most frequent class remaining)
rs = RuleSelection(test_data, classifiers)
rs.call(attribute_to_classify)

# Retrieve the rules in the classifier in order of their heuristic value
classifiers = rs.classifiers

# Map back values that have been discretized, back to the range their original values fall in
unmap_classifiers_with_binning_refereces(classifiers, binning_references)


#########################
# Stop the "stopwatch" to measure the time taken to train the classifier
#########################

elapsed_time = time.process_time() - start
print(">>>>> Time to determine assoc rules: " + str(elapsed_time) + "secs")
print()


#########################
# Print out the classifiers generated
#########################

pprint(classifiers)
print(len(classifiers))
print()


#########################
# Step 4: Evaluating the accuracy of the classifier
#########################

# Load the data with the guessed class into the analyzer
aa = AccuracyAnalyzer(rs.data)

# Calculate the percentage of the guesses that are correct when compared to the actual classes
accuracy = aa.call(attribute_to_classify)
print(accuracy)
